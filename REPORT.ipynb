{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation Project\n",
    "\n",
    "## Project's goal\n",
    "\n",
    "In this project, **the goal is to train an agent to navigate a large square world virtual world and collect as many yellow bananas as possible while avoiding blue bananas**. The project is implemented by means\n",
    "\n",
    "of Deep Q-Network.\n",
    "\n",
    "![In Project 1, train an agent to navigate a large world.](./navigation.gif)\n",
    "\n",
    "## Environment details\n",
    "\n",
    "The environment is based on [Unity ML-agents](https://github.com/Unity-Technologies/ml-agents)\n",
    "\n",
    "Note: The project environment provided by Udacity is similar to, but not identical to the Banana Collector environment on the Unity ML-Agents GitHub page.\n",
    "\n",
    "A reward of +1 is provided for collecting a yellow banana, and a reward of -1 is provided for collecting a blue banana. Thus, the goal of the agent is to collect as many yellow bananas as possible while avoiding blue bananas.\n",
    "\n",
    "The state space has 37 dimensions and contains the agent's velocity, along with ray-based perception of objects around the agent's forward direction. \n",
    "\n",
    "Given this information, the agent has to learn how to best select actions. Four discrete actions are available, corresponding to:\n",
    "\n",
    "- 0 - move forward.\n",
    "- 1 - move backward.\n",
    "- 2 - turn left.\n",
    "- 3 - turn right.\n",
    "\n",
    "The task is episodic, and **in order to solve the environment, the agent must get an average score of +13 over 100 consecutive episodes.**\n",
    "\n",
    "\n",
    "## Agent Implementation\n",
    "\n",
    "### Deep Q-Networks\n",
    "\n",
    "This project implements a *Value Based* method called [Deep Q-Networks](https://deepmind.com/research/dqn/). \n",
    "\n",
    "Deep Q Learning combines 2 approaches :\n",
    "- A Reinforcement Learning method called [Q Learning](https://en.wikipedia.org/wiki/Q-learning) (aka SARSA max)\n",
    "- A Deep Neural Network to learn a Q-table approximation (action-values)\n",
    "\n",
    "- This includes a pair of neural networks. They are local and target network. They are trained in such a way that both are identical with the only difference in the weights that are updated. The local network is used to retrieve Q values while the target is used for updating. The aim is to store the Q targets temporarily so we donâ€™t need a moving target.\n",
    "\n",
    "This implementation includes the 2 major training improvements:\n",
    "\n",
    "- Reinforcement learning is known to be unstable or even to diverge when a nonlinear function approximator such as a    neural network is used to represent the action-value (also known as Q) function20. This instability has several causes: the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and therefore change the data distribution, and the correlationsbetween the action-values and the target values .\n",
    "- We address these instabilities with a novel variant of Q-learning, which uses two key ideas. First, we used a biologically inspired mechanism termed experience replay that randomizes over the data, thereby removing correlations in the observation sequence and smoothing over changes in the data distribution. Second, we used an iterative update that adjusts the action-values towards target values that are only periodically updated, thereby reducing correlations with the target.([Nature publication : \"Human-level control through deep reinforcement learning (2015)\"](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf))\n",
    "\n",
    "\n",
    "### Code implementation\n",
    "\n",
    "The code used here is derived from the \"Lunar Lander\" tutorial from the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893), and has been slightly adjusted for being used with the banana environment.\n",
    "\n",
    "The code consist of :\n",
    "\n",
    "- model.py : In this python file, a PyTorch QNetwork class is implemented. This is a regular fully connected Deep Neural Network using the [PyTorch Framework](https://pytorch.org/docs/0.4.0/). This network will be trained to predict the action to perform depending on the environment observed states. This Neural Network is used by the DQN agent and is composed of :\n",
    "  - the input layer which size depends of the state_size parameter passed in the constructor\n",
    "  - 3 hidden fully connected layers.\n",
    "  - the output layer which size depends of the action_size parameter passed in the constructor\n",
    "- dqn_agent.py :\n",
    "  - In this python file, a DQN agent and a Replay Buffer memory used by the DQN agent) are defined.\n",
    "        \n",
    "- DQN_Banana_Navigation.ipynb : This Jupyter notebooks allows to train the agent. More in details it allows to :\n",
    "  - Import the Necessary Packages \n",
    "  - Examine the State and Action Spaces\n",
    "  - Take Random Actions in the Environment (No display)\n",
    "  - Train an agent using DQN\n",
    "  - Plot the scores\n",
    "\n",
    "### DQN parameters and results\n",
    "\n",
    "The DQN agent uses the following parameters values (defined in dqn_agent.py)\n",
    "\n",
    "```\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size \n",
    "GAMMA = 0.995           # discount factor \n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "```\n",
    "\n",
    "The Neural Networks use the following architecture :\n",
    "\n",
    "```\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "```\n",
    "\n",
    "The Neural Networks use the Adam optimizer with a learning rate LR=5e-4 and are trained using a BATCH_SIZE=64\n",
    "\n",
    "Given the chosen architecture and parameters, our results are :\n",
    "\n",
    "![Training logs](./log.png)\n",
    "\n",
    "![Score evolution during the training](./plot.png)\n",
    "\n",
    "**These results meets the project's expectation as the agent is able to receive an average reward (over 100 episodes) of at least +13, and in 401 episodes only** (In comparison, according to Udacity's solution code for the project, their agent was benchmarked to be able to solve the project in fewer than 1800 episodes)\n",
    "**This was possible because of the additional layer that was added to the neural net**\n",
    "\n",
    "### Ideas for future work\n",
    "\n",
    "As discussed in the Udacity Course, a further evolution to this project would be to train the agent directly from the environment's observed raw pixels instead of using the environment's internal states (37 dimensions)\n",
    "\n",
    "To do so a [Convolutional Neural Network](https://en.wikipedia.org/wiki/Convolutional_neural_network) would be added at the input of the network in order to process the raw pixels values (after some little preprocessing like rescaling the image size, converting RGB to gray scale, ...).Also , if the convolutinal networks are used in Dueling DQN with prioritized experience replay . The agent would be able to differentiate important transitions and might lead to much better accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
